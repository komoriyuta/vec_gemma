{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2d74aa0",
   "metadata": {},
   "source": [
    "# Training Notebook\n",
    "\n",
    "この Notebook は、もともとの `train.py` のコードを Notebook 用にセル分割したものです。以下の各セルを順次実行することで、学習プロセスを開始できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c578e76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yut/sail-develop/vec_gemma/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import random\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm.auto import tqdm\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "from datasets import disable_progress_bars\n",
    "disable_progress_bars()\n",
    "from datasets import load_dataset\n",
    "\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "from schedulefree import RAdamScheduleFree\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from gemma.config import GemmaConfig, get_model_config\n",
    "from gemma.model import GemmaForCausalLM\n",
    "from VAEs.LinearVAE import LinearVAE\n",
    "from VAEs.VAE import VAE\n",
    "from VAEs.Perceptron import Perceptron\n",
    "\n",
    "torch.set_float32_matmul_precision('high')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d197ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingConfig:\n",
    "    def __init__(self):\n",
    "        # データ設定\n",
    "        self.batch_size = 4\n",
    "        self.max_seq_len = 256\n",
    "        self.num_workers = 4\n",
    "        \n",
    "        # 最適化設定\n",
    "        self.lr = 1e-4\n",
    "        self.num_epochs = 10\n",
    "        self.grad_accum_steps = 4\n",
    "        self.beta_init = 0.00\n",
    "        self.beta_max = 0\n",
    "        self.beta_step = 1e-6\n",
    "        self.crop_lambda = 0.2\n",
    "\n",
    "        # モデル設定\n",
    "        self.bert_model_name = \"cl-nagoya/ruri-large\"\n",
    "        self.gemma_model_size = \"2b-v2\"\n",
    "        self.vae_hidden_dim = 512\n",
    "        self.vae_latent_dim = 128\n",
    "        \n",
    "        # 生成設定\n",
    "        self.sample_interval = 1000\n",
    "        self.num_samples = 3\n",
    "        self.max_gen_length = 100\n",
    "        self.generation_temp = 0.1\n",
    "        self.generation_top_p = 0.2\n",
    "        self.generation_top_k = 2\n",
    "        \n",
    "        self.ckpt_interval = 5000\n",
    "        # パス設定\n",
    "        self.log_dir = f\"./logs/{datetime.datetime.now()}/\"\n",
    "        self.checkpoint_dir = \"./checkpoints\"\n",
    "        self.dataset_path = \"AhmedSSabir/Japanese-wiki-dump-sentence-dataset\"\n",
    "        \n",
    "        # 初期化\n",
    "        self._setup_directories()\n",
    "        \n",
    "    def _setup_directories(self):\n",
    "        Path(self.log_dir).mkdir(parents=True, exist_ok=True)\n",
    "        Path(self.checkpoint_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "def setup_logging(log_dir):\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "        handlers=[\n",
    "            logging.FileHandler(os.path.join(log_dir, \"training.log\")),\n",
    "            logging.StreamHandler()\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "def prepare_dataset(config):\n",
    "    def collate_fn(batch):\n",
    "        return [item['text'] for item in batch]\n",
    "    \n",
    "    dataset = load_dataset(config.dataset_path, cache_dir=\"./.datasets\")\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        dataset['train'].with_format(\"torch\"),\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn,\n",
    "        num_workers=config.num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    return train_loader\n",
    "\n",
    "\n",
    "def initialize_models(config, device):\n",
    "    # BERTモデル\n",
    "    bert_model = SentenceTransformer(config.bert_model_name)\n",
    "    bert_model.requires_grad_(False)\n",
    "    bert_model = bert_model.to(device)\n",
    "    \n",
    "    # Gemmaモデル\n",
    "    snapshot_dir = snapshot_download(repo_id='google/gemma-2-2b-jpn-it-pytorch')\n",
    "\n",
    "    # Ensure that the tokenizer is present\n",
    "    tokenizer_path = os.path.join(snapshot_dir, 'tokenizer.model')\n",
    "    assert os.path.isfile(tokenizer_path), 'Tokenizer not found!'\n",
    "\n",
    "    # Ensure that the checkpoint is present\n",
    "    ckpt_path = os.path.join(snapshot_dir, f'model.ckpt')\n",
    "    assert os.path.isfile(ckpt_path), 'PyTorch checkpoint not found!'\n",
    "    \n",
    "\n",
    "    gemma_model_config = get_model_config(\"2b-v2\")\n",
    "    gemma_model_config.tokenizer = tokenizer_path\n",
    "    \n",
    "    # Instantiate the model and load the weights.\n",
    "    torch.set_default_dtype(gemma_model_config.get_dtype())\n",
    "    gemma_model = GemmaForCausalLM(gemma_model_config)\n",
    "    gemma_model.requires_grad_(False)\n",
    "    gemma_model.load_weights(ckpt_path)\n",
    "    gemma_model = gemma_model.to(device).eval()\n",
    "    \n",
    "    # VAEモデル\n",
    "    vae_model = Perceptron(\n",
    "        bert_model.get_sentence_embedding_dimension(),\n",
    "        gemma_model.config.hidden_size,\n",
    "        hidden_dim=config.vae_hidden_dim,\n",
    "        latent_dim=config.vae_latent_dim\n",
    "    ).to(device)\n",
    "    \n",
    "    return bert_model, gemma_model, vae_model\n",
    "\n",
    "\n",
    "def generate_and_log_samples(vae_model, bert_model, gemma_model, device, writer, global_step, config):\n",
    "    vae_model.eval()\n",
    "    sample_texts = [\n",
    "        \"人工知能の未来について\",\n",
    "        \"量子コンピュータの可能性\",\n",
    "        \"ディープラーニングの応用分野\",\n",
    "        \"自然言語処理の最新動向\",\n",
    "        \"ロボット工学の進化\",\n",
    "        \"一時期所長となる。\",\n",
    "        \"京都府京都市に生まれる。\",\n",
    "        \"卒業後は文章を書く仕事がしたいと1994年に報知新聞社にスポーツ記者として勤務し、高校野球やゴルフを取材する。\",\n",
    "        \"その後、全てをリセットするためにタンザニア・ダルエスサラーム大学に留学し、スワヒリ語科で学ぶ。\",\n",
    "        \"29歳の時に新人賞の最終候補に残るが、その後結婚し、出産したことで小説を書く余裕を無くしてしまう。\",\n",
    "        \"本形式は、192形を改称して生まれた形式である。\",\n",
    "        \"ただし、現実にはこの変更を受けたのは数両程度である。\"\n",
    "    ]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        prep_texts = [\"文章: \" + text for text in sample_texts]\n",
    "        \n",
    "        # BERTで埋め込みを取得\n",
    "        bert_embeddings = bert_model.encode(\n",
    "            prep_texts, \n",
    "            convert_to_tensor=True,\n",
    "            device=device,\n",
    "            show_progress_bar=False\n",
    "        ).to(torch.bfloat16)\n",
    "        \n",
    "        # VAEで埋め込みを変換\n",
    "        vae_output, _, _ = vae_model(bert_embeddings)\n",
    "        \n",
    "        # Gemmaでテキスト生成\n",
    "        generated_texts = []\n",
    "        for i in range(len(sample_texts)):\n",
    "            try:\n",
    "                embedding = vae_output[i].unsqueeze(0).unsqueeze(0)\n",
    "                generated = gemma_model.generate_with_initial_embedding(\n",
    "                    initial_embedding=embedding,\n",
    "                    device=device,\n",
    "                    output_len=config.max_gen_length,\n",
    "                    temperature=config.generation_temp,\n",
    "                    top_p=config.generation_top_p,\n",
    "                    top_k=config.generation_top_k,\n",
    "                    instructions=(\"<start_of_turn>user\\n \\\"\", \"\\\"\\nこれをそのままの意味で出力してください。<end_of_turn>\\n<start_of_turn>model\\n\")\n",
    "                )\n",
    "                generated_texts.append(generated[0])\n",
    "            except Exception as e:\n",
    "                generated_texts.append(f\"生成エラー: {str(e)}\")\n",
    "        \n",
    "        # ログに記録\n",
    "        log_text = \"\\n\\n\".join([\n",
    "            f\"入力: {sample_texts[i]}\\n生成結果: {generated_texts[i]}\" \n",
    "            for i in range(len(sample_texts))\n",
    "        ])\n",
    "        \n",
    "        writer.add_text(\"生成サンプル\", log_text, global_step)\n",
    "        logging.info(f\"\\n=== ステップ {global_step} の生成サンプル ===\\n{log_text}\\n\")\n",
    "    \n",
    "    vae_model.train()\n",
    "\n",
    "\n",
    "def train(config, vae_model, bert_model, gemma_model, optimizer, device, start_epoch=0, initial_beta=0.1):\n",
    "    writer = SummaryWriter(config.log_dir)\n",
    "    setup_logging(config.log_dir)\n",
    "    \n",
    "    instructions = [\n",
    "        (\"<start_of_turn>user以下の内容をそのまま再現してください:\\\"\", \"\\\"\\n<end_of_turn><start_of_turn>model\\n\"),\n",
    "    ]\n",
    "\n",
    "    best_loss = float('inf')\n",
    "    beta = initial_beta\n",
    "    optimizer.train()\n",
    "\n",
    "    for epoch in range(start_epoch, 100):\n",
    "        logging.info(f\"エポック {epoch+1}/{config.num_epochs}\")\n",
    "        train_loader = prepare_small_dataset(config)\n",
    "        total_steps = len(train_loader)\n",
    "        progress_bar = tqdm(\n",
    "            train_loader,\n",
    "            total=total_steps,\n",
    "            desc=f\"エポック {epoch+1}/{config.num_epochs}\",\n",
    "            bar_format=\"{l_bar}{bar:20}{r_bar}{bar:-10b}\",\n",
    "            dynamic_ncols=True\n",
    "        )\n",
    "        for step, batch_texts in enumerate(progress_bar):\n",
    "            with torch.no_grad():\n",
    "                rep_texts = [\"文章: \" + text for text in batch_texts]\n",
    "                bert_embeddings = bert_model.encode(\n",
    "                    rep_texts, \n",
    "                    convert_to_tensor=True,\n",
    "                    device=device,\n",
    "                    show_progress_bar=False\n",
    "                ).to(torch.bfloat16)\n",
    "            \n",
    "            with torch.autocast(device_type='cuda', dtype=torch.bfloat16):\n",
    "                vae_output, mu, logvar = vae_model(bert_embeddings)\n",
    "                \n",
    "                kl_div = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "                kl_div_per_batch = kl_div / bert_embeddings.size(0)\n",
    "                kl_div = torch.maximum(\n",
    "                    kl_div_per_batch,\n",
    "                    torch.tensor(config.crop_lambda, device=kl_div.device, dtype=kl_div.dtype)\n",
    "                )\n",
    "                \n",
    "                recon_loss = gemma_model.forward_teacher_forcing(\n",
    "                    vae_output, \n",
    "                    batch_texts,\n",
    "                    max_seq_len=config.max_seq_len,\n",
    "                    instructions=instructions[random.randint(0, len(instructions)-1)],\n",
    "                )\n",
    "                \n",
    "                loss = recon_loss + beta * kl_div\n",
    "\n",
    "            loss.backward()\n",
    "            \n",
    "            if (step + 1) % config.grad_accum_steps == 0:\n",
    "                optimizer.step()\n",
    "            \n",
    "            writer.add_scalar(\"損失/訓練\", loss.item(), epoch*len(train_loader)+step)\n",
    "            writer.add_scalar(\"損失/再構成\", recon_loss.item(), epoch*len(train_loader)+step)\n",
    "            writer.add_scalar(\"損失/KL\", kl_div.item(), epoch*len(train_loader)+step)\n",
    "            writer.add_scalar(\"Beta\", beta, epoch*len(train_loader)+step)\n",
    "            \n",
    "            progress_bar.set_postfix({\n",
    "                \"loss\": f\"{loss.item():.4f}\",\n",
    "                \"recon\": f\"{recon_loss.item():.4f}\",\n",
    "                \"kl\": f\"{kl_div.item():.4f}\",\n",
    "                \"beta\": f\"{beta:.4f}\"\n",
    "            })\n",
    "            optimizer.eval()\n",
    "\n",
    "            #if step % config.sample_interval == 0:\n",
    "            #    generate_and_log_samples(\n",
    "            ##        vae_model=vae_model,\n",
    "            #        bert_model=bert_model,\n",
    "            #        gemma_model=gemma_model,\n",
    "            #        device=device,\n",
    "            #        writer=writer,\n",
    "            #        global_step=epoch*len(train_loader)+step,\n",
    "            #        config=config,\n",
    "            #    )\n",
    "            if step % config.ckpt_interval == 0:\n",
    "                checkpoint = {\n",
    "                    \"epoch\": epoch-1,\n",
    "                    \"model_state\": vae_model.state_dict(),\n",
    "                    \"optimizer_state\": optimizer.state_dict(),\n",
    "                    \"loss\": loss.item(),\n",
    "                    \"beta\": beta,\n",
    "                    \"settings\": config.__dict__\n",
    "                }\n",
    "                torch.save(checkpoint, os.path.join(config.checkpoint_dir, f\"checkpoint_epoch_{epoch+1}_step_{datetime.datetime.now()}_{step}.pt\"))\n",
    "                torch.save(vae_model.state_dict(), os.path.join(config.checkpoint_dir, \"latest_model.pt\"))\n",
    "            optimizer.train()\n",
    "            beta = min(config.beta_max, beta + config.beta_step)\n",
    "        \n",
    "        #generate_and_log_samples(\n",
    "        #    vae_model=vae_model,\n",
    "        #    bert_model=bert_model,\n",
    "        #    gemma_model=gemma_model,\n",
    "        #    device=device,\n",
    "        #    writer=writer,\n",
    "        #    global_step=(epoch+1)*len(train_loader),\n",
    "        #    config=config\n",
    "        #)\n",
    "        \n",
    "        #checkpoint = {\n",
    "        #    \"epoch\": epoch,\n",
    "        #    \"model_state\": vae_model.state_dict(),\n",
    "        #    \"optimizer_state\": optimizer.state_dict(),\n",
    "        #    \"loss\": loss.item(),\n",
    "        #    \"beta\": beta,\n",
    "        #    \"settings\": config.__dict__\n",
    "        #}\n",
    "        \n",
    "        #torch.save(checkpoint, os.path.join(config.checkpoint_dir, f\"checkpoint_epoch_{epoch+1}_{datetime.datetime.now()}.pt\"))\n",
    "        \n",
    "        if loss.item() < best_loss:\n",
    "            best_loss = loss.item()\n",
    "            torch.save(vae_model.state_dict(), os.path.join(config.checkpoint_dir, f\"best_model{datetime.datetime.now()}.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72b7c2a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 4 files: 100%|██████████| 4/4 [00:00<00:00, 70492.50it/s]\n",
      "/home/yut/sail-develop/vec_gemma/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1132: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at ../aten/src/ATen/native/Copy.cpp:308.)\n",
      "  return self._apply(lambda t: t.type(dst_type))\n"
     ]
    }
   ],
   "source": [
    "# --- 初期化＆チェックポイントの読み込み（必要に応じて） ---\n",
    "\n",
    "config = TrainingConfig()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "bert_model, gemma_model, vae_model = initialize_models(config, device)\n",
    "vae_model.type(torch.bfloat16)\n",
    "gemma_model.type(torch.bfloat16)\n",
    "optimizer = RAdamScheduleFree(vae_model.parameters(), lr=config.lr)\n",
    "\n",
    "# チェックポイントから再開したい場合は、以下 resume にパスを設定してください。なければ None のままでOK\n",
    "resume = None  # 例: \"./checkpoints/your_checkpoint.pt\"\n",
    "if resume:\n",
    "    checkpoint = torch.load(resume, map_location=device)\n",
    "    vae_model.load_state_dict(checkpoint[\"model_state\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer_state\"])\n",
    "    start_epoch = checkpoint[\"epoch\"] + 1\n",
    "    beta = checkpoint[\"beta\"]\n",
    "    if \"settings\" in checkpoint:\n",
    "        config.__dict__.update(checkpoint[\"settings\"])\n",
    "    logging.info(f\"Resuming training from epoch {start_epoch}\")\n",
    "else:\n",
    "    start_epoch = 0\n",
    "    beta = config.beta_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ee8069a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_small_dataset(config):\n",
    "    def collate_fn(batch):\n",
    "        return [item['text'] for item in batch]\n",
    "    \n",
    "    dataset = load_dataset(config.dataset_path, cache_dir=\"./.datasets\")\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        dataset['train'].shuffle(seed=42).select(range(100)),\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn,\n",
    "        num_workers=config.num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    return train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa00f46a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.1552734375\n",
      "fc1.weight 25.25\n",
      "fc1.bias 0.9765625\n",
      "fc2.weight 8.6875\n",
      "fc2.bias 2.453125\n"
     ]
    }
   ],
   "source": [
    "text = \"あああああああ\"\n",
    "bert_emb = bert_model.encode([text], convert_to_tensor=True, device=device).to(torch.bfloat16)\n",
    "vae_output, _, _ = vae_model(bert_emb)\n",
    "gemma_model.train()\n",
    "\n",
    "loss = gemma_model.forward_teacher_forcing(\n",
    "    vae_output, [text], max_seq_len=config.max_seq_len,\n",
    "    instructions=(\"<start_of_turn>user\\n \\\"\", \"\\\"\\nこれをそのままの意味で出力してください。<end_of_turn>\\n<start_of_turn>model\\n\")\n",
    ")\n",
    "print(loss.item())\n",
    "from torchviz import make_dot\n",
    "\n",
    "# 例: loss の計算が終わった後\n",
    "dot = make_dot(loss, params=dict(vae_model.named_parameters()))\n",
    "dot.render(\"loss_graph\", format=\"svg\")  # loss_graph.png として保存される\n",
    "\n",
    "with torch.autograd.set_detect_anomaly(True):\n",
    "    loss.backward()\n",
    "    for name, param in vae_model.named_parameters():\n",
    "        if param.grad is not None:\n",
    "            print(name, param.grad.norm().item())\n",
    "        else:\n",
    "            print(name, \"None\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2c6fb30e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-11 00:44:31,386 - INFO - エポック 1/10\n",
      "エポック 1/10:   0%|                    | 0/25 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 18.00 MiB. GPU 0 has a total capacity of 15.58 GiB of which 24.25 MiB is free. Including non-PyTorch memory, this process has 14.63 GiB memory in use. Of the allocated memory 14.20 GiB is allocated by PyTorch, and 276.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# --- トレーニング開始 ---\u001b[39;00m\n\u001b[1;32m      2\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[0;32m----> 3\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvae_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvae_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbert_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbert_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgemma_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgemma_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstart_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitial_beta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 216\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(config, vae_model, bert_model, gemma_model, optimizer, device, start_epoch, initial_beta)\u001b[0m\n\u001b[1;32m    210\u001b[0m     kl_div_per_batch \u001b[38;5;241m=\u001b[39m kl_div \u001b[38;5;241m/\u001b[39m bert_embeddings\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    211\u001b[0m     kl_div \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmaximum(\n\u001b[1;32m    212\u001b[0m         kl_div_per_batch,\n\u001b[1;32m    213\u001b[0m         torch\u001b[38;5;241m.\u001b[39mtensor(config\u001b[38;5;241m.\u001b[39mcrop_lambda, device\u001b[38;5;241m=\u001b[39mkl_div\u001b[38;5;241m.\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mkl_div\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m    214\u001b[0m     )\n\u001b[0;32m--> 216\u001b[0m     recon_loss \u001b[38;5;241m=\u001b[39m \u001b[43mgemma_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_teacher_forcing\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvae_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_texts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_seq_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_seq_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[43m        \u001b[49m\u001b[43minstructions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minstructions\u001b[49m\u001b[43m[\u001b[49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandint\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minstructions\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    223\u001b[0m     loss \u001b[38;5;241m=\u001b[39m recon_loss \u001b[38;5;241m+\u001b[39m beta \u001b[38;5;241m*\u001b[39m kl_div\n\u001b[1;32m    225\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/sail-develop/vec_gemma/src/gemma/model.py:971\u001b[0m, in \u001b[0;36mGemmaForCausalLM.forward_teacher_forcing\u001b[0;34m(self, vae_embedding, target_texts, max_seq_len, instructions)\u001b[0m\n\u001b[1;32m    968\u001b[0m     kv_caches\u001b[38;5;241m.\u001b[39mappend((k_cache, v_cache))\n\u001b[1;32m    970\u001b[0m \u001b[38;5;66;03m# 11. モデルへ順伝播\u001b[39;00m\n\u001b[0;32m--> 971\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    973\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfreqs_cis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfreqs_cis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    974\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkv_write_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpositions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    975\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkv_caches\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkv_caches\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    976\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcombined_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    977\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    979\u001b[0m \u001b[38;5;66;03m# 12. ロジットの計算\u001b[39;00m\n\u001b[1;32m    980\u001b[0m embed_weight \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedder\u001b[38;5;241m.\u001b[39mweight\n",
      "File \u001b[0;32m~/sail-develop/vec_gemma/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/sail-develop/vec_gemma/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/sail-develop/vec_gemma/src/gemma/model.py:494\u001b[0m, in \u001b[0;36mGemmaModel.forward\u001b[0;34m(self, hidden_states, freqs_cis, kv_write_indices, kv_caches, mask)\u001b[0m\n\u001b[1;32m    492\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers)):\n\u001b[1;32m    493\u001b[0m     layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers[i]\n\u001b[0;32m--> 494\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfreqs_cis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfreqs_cis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkv_write_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkv_write_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkv_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkv_caches\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    501\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(hidden_states)\n\u001b[1;32m    502\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m~/sail-develop/vec_gemma/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/sail-develop/vec_gemma/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/sail-develop/vec_gemma/src/gemma/model.py:454\u001b[0m, in \u001b[0;36mGemma2DecoderLayer.forward\u001b[0;34m(self, hidden_states, freqs_cis, kv_write_indices, kv_cache, mask)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpre_feedforward_layernorm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    453\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpre_feedforward_layernorm(hidden_states)\n\u001b[0;32m--> 454\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_feedforward_layernorm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    456\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_feedforward_layernorm(hidden_states)\n",
      "File \u001b[0;32m~/sail-develop/vec_gemma/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/sail-develop/vec_gemma/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/sail-develop/vec_gemma/src/gemma/model.py:208\u001b[0m, in \u001b[0;36mGemmaMLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    206\u001b[0m gate \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgate_proj(x)\n\u001b[1;32m    207\u001b[0m gate \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mgelu(gate, approximate\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtanh\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 208\u001b[0m up \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mup_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    209\u001b[0m fuse \u001b[38;5;241m=\u001b[39m gate \u001b[38;5;241m*\u001b[39m up\n\u001b[1;32m    210\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown_proj(fuse)\n",
      "File \u001b[0;32m~/sail-develop/vec_gemma/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/sail-develop/vec_gemma/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/sail-develop/vec_gemma/src/gemma/model.py:136\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquant:\n\u001b[1;32m    135\u001b[0m     weight \u001b[38;5;241m=\u001b[39m weight \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight_scaler\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 136\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 18.00 MiB. GPU 0 has a total capacity of 15.58 GiB of which 24.25 MiB is free. Including non-PyTorch memory, this process has 14.63 GiB memory in use. Of the allocated memory 14.20 GiB is allocated by PyTorch, and 276.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# --- トレーニング開始 ---\n",
    "torch.cuda.empty_cache()\n",
    "train(\n",
    "    config=config,\n",
    "    vae_model=vae_model,\n",
    "    bert_model=bert_model,\n",
    "    gemma_model=gemma_model,\n",
    "    optimizer=optimizer,\n",
    "    device=device,\n",
    "    start_epoch=start_epoch,\n",
    "    initial_beta=beta\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1913f6",
   "metadata": {},
   "source": [
    "学習が開始されました。ログや TensorBoard を確認してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2b1242f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 39.45it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['\"\\n\\n\"\\n\\n\"\\n\\n\"\\n\\n\"\\n\\n\"\\n\\n\"\\n\\n\"\\n\\n\"\\n\\n\"\\n\\n\"\\n\\n\"\\n\\n\"\\n\\n\"\\n\\n\"\\n\\n\"\\n\\n\"\\n\\n\"\\n\\n\"\\n\\n\"\\n\\n\"\\n\\n\"\\n\\n\"\\n\\n\"\\n\\n\"\\n\\n\"\\n\\n\"\\n\\n\"\\n\\n\"\\n\\n\"\\n\\n\"\\n\\n\"\\n\\n\"\\n\\n\"\\n\\n\"\\n\\n\"\\n\\n\"\\n\\n\"\\n\\n\"\\n\\n\"\\n\\n\"\\n\\n\"\\n\\n\"\\n\\n\"\\n\\n\"\\n\\n\"\\n\\n\"\\n\\n\"\\n\\n\"\\n\\n\"\\n\\n']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_loader = prepare_small_dataset(config)\n",
    "text=small_loader.dataset[0]['text']\n",
    "\n",
    "bert_emb = bert_model.encode([text], convert_to_tensor=True, device=device).to(torch.bfloat16)\n",
    "vae_output, _, _ = vae_model(bert_emb)\n",
    "gemma_model.generate_with_initial_embedding(\n",
    "    initial_embedding=vae_output,\n",
    "    device=device,\n",
    "    output_len=config.max_gen_length,\n",
    "    temperature=config.generation_temp,\n",
    "    top_p=config.generation_top_p,\n",
    "    top_k=2,\n",
    "    instructions=(\"<start_of_turn>user以下の内容をそのまま再現してください:\\\"\", \"\\\"\\n<end_of_turn><start_of_turn>model\\n\")\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
